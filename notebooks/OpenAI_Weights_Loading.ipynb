{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0114db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b658709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c9ead0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT.gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "114afb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hites\\OneDrive\\Desktop\\GPT\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ../gpt2_355M\\355M\\checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hites\\OneDrive\\Desktop\\GPT\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ../gpt2_355M\\355M\\encoder.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hites\\OneDrive\\Desktop\\GPT\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ../gpt2_355M\\355M\\hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hites\\OneDrive\\Desktop\\GPT\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ../gpt2_355M\\355M\\model.ckpt.data-00000-of-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hites\\OneDrive\\Desktop\\GPT\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ../gpt2_355M\\355M\\model.ckpt.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hites\\OneDrive\\Desktop\\GPT\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ../gpt2_355M\\355M\\model.ckpt.meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hites\\OneDrive\\Desktop\\GPT\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ../gpt2_355M\\355M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"355M\", models_dir=\"../gpt2_355M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0992ab00",
   "metadata": {},
   "source": [
    "Both settings and params are Python dictionaries. The settings dictionary stores the LLM\n",
    "architecture settings similarly to our manually defined GPT_CONFIG_124M settings. \n",
    "\n",
    "The\n",
    "params dictionary contains the actual weight tensors. \n",
    "\n",
    "    \n",
    "- printed the\n",
    "dictionary keys because printing the weight contents would take up too much screen space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "045bc026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0115168   0.00311915 -0.00729894 ... -0.05262156 -0.17569277\n",
      "   0.02565791]\n",
      " [-0.00861426  0.06360211 -0.01822355 ... -0.01364703 -0.12153847\n",
      "   0.05352487]\n",
      " [ 0.05854857  0.06891199  0.02622696 ... -0.10057542 -0.19788682\n",
      "  -0.0039184 ]\n",
      " ...\n",
      " [ 0.00162342 -0.04411932 -0.0517492  ... -0.10079621 -0.00865952\n",
      "   0.02637872]\n",
      " [-0.14374605 -0.04632217 -0.00650705 ...  0.07464293 -0.04721651\n",
      "  -0.03829013]\n",
      " [ 0.02065966 -0.01334631 -0.02586888 ...  0.03886637 -0.00233481\n",
      "   0.00107106]]\n",
      "Token embedding weight tensor dimensions: (50257, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d430e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT.GPT_Model import GPT_CONFIG_124M, GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97c7fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e57994d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50257,\n",
       " 'context_length': 1024,\n",
       " 'emb_dim': 768,\n",
       " 'n_heads': 12,\n",
       " 'n_layers': 12,\n",
       " 'drop_rate': 0.1,\n",
       " 'qkv_bias': True}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "NEW_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b9d3080",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG = {'vocab_size': 50257,\n",
    " 'context_length': 1024,\n",
    " 'emb_dim': 1024,\n",
    " 'n_heads': 16,\n",
    " 'n_layers': 24,\n",
    " 'drop_rate': 0.1,\n",
    " 'qkv_bias': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f66c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4d953",
   "metadata": {},
   "source": [
    "By default, the GPTModel instance is initialized with random weights for pretraining.\n",
    "\n",
    "The last step to using OpenAI's model weights is to override these random weights with the weights we loaded into the params dictionary.\n",
    "\n",
    "For this, we will first define a small assign utility function that checks whether two tensors or arrays (left and right) have the same dimensions or shape and returns the right tensor as trainable PyTorch parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5018fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a140e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def load_weights_into_gpt(model, params):\n",
    "    # Embedding Layers\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    # Transformer Blocks (Loop over all layers)\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        # Attention Weights (Q, K, V)\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_block[b].att.W_query.weight = assign(\n",
    "            gpt.trf_block[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_block[b].att.W_key.weight = assign(\n",
    "            gpt.trf_block[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_block[b].att.W_value.weight = assign(\n",
    "            gpt.trf_block[b].att.W_value.weight, v_w.T)\n",
    "        \n",
    "        # Attention Biases (Q, K, V)\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_block[b].att.W_query.bias = assign(\n",
    "            gpt.trf_block[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_block[b].att.W_key.bias = assign(\n",
    "            gpt.trf_block[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_block[b].att.W_value.bias = assign(\n",
    "            gpt.trf_block[b].att.W_value.bias, v_b)\n",
    "        \n",
    "        # Attention Output Projection\n",
    "        gpt.trf_block[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_block[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_block[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_block[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "        \n",
    "        # Feedforward (MLP) Layers\n",
    "        gpt.trf_block[b].ffn.layers[0].weight = assign(\n",
    "            gpt.trf_block[b].ffn.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_block[b].ffn.layers[0].bias = assign(\n",
    "            gpt.trf_block[b].ffn.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_block[b].ffn.layers[2].weight = assign(\n",
    "            gpt.trf_block[b].ffn.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_block[b].ffn.layers[2].bias = assign(\n",
    "            gpt.trf_block[b].ffn.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "        \n",
    "        # LayerNorm Parameters\n",
    "        gpt.trf_block[b].norm1.scale = assign(\n",
    "            gpt.trf_block[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_block[b].norm1.shift = assign(\n",
    "            gpt.trf_block[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_block[b].norm2.scale = assign(\n",
    "            gpt.trf_block[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_block[b].norm2.shift = assign(\n",
    "            gpt.trf_block[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "    \n",
    "    # Final LayerNorm and Output Head\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb7e280",
   "metadata": {},
   "source": [
    "matching the weights from OpenAI's implementation with our GPTModel implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f857b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_into_gpt(gpt, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c87389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "gpt.to(device=device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf739ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b33fe7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "from GPT.Text_Generation import generate \n",
    "from GPT.Tokenization import text_to_tokens, token_to_text\n",
    "import tiktoken \n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "text = \"Computer\"\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt, \n",
    "    idx = text_to_tokens(text, tokenizer = tokenizer).to(device), \n",
    "    max_new_tokens=50, \n",
    "    context_size=NEW_CONFIG[\"context_length\"], \n",
    "    top_k=50, \n",
    "    temperature=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a64d34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output_text: Computer\" will create a new local copy of SQL Server 2010 or later using Active Directory Users and Computers permissions to create these credentials:\n",
      "\n",
      "<CS01> <PSC1>: Set <SYSTEMLOGINID> <SUBCONSE\n"
     ]
    }
   ],
   "source": [
    "print(f\"Output_text: {token_to_text(token_ids, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a85024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
