{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b188c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "import ssl\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "    ssl_context = ssl.create_default_context()\n",
    "    ssl_context.check_hostname = False\n",
    "    ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url, context=ssl_context) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f891844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Edit the following sentence for grammar.',\n",
       " 'input': 'He go to the park every day.',\n",
       " 'output': 'He goes to the park every day.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f191512",
   "metadata": {},
   "source": [
    "## **CONVERTING INSTRUCTIONS INTO ALPACA FORMAT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb800ea",
   "metadata": {},
   "source": [
    "### What is Alpaca ?\n",
    "\n",
    "Stanford Alpaca is a language model developed by researchers at Stanford University in March 2023.\n",
    "\n",
    "It’s based on Meta’s LLaMA (Large Language Model Meta AI) models, which are large foundational language models\n",
    "\n",
    "Stanford researchers wanted to create a smaller, more accessible version of large language models, which is fine-tuned to follow instructions well (similar to ChatGPT).\n",
    "\n",
    "Their main goals were:\n",
    "\n",
    "- To show that instruction-following models can be made cheaply.\n",
    "\n",
    "- To make it easier for researchers and developers to experiment with these models.\n",
    "\n",
    "⚙️ How did they make Alpaca?\n",
    "- 1️⃣ They started with LLaMA 7B, a 7-billion-parameter model from Meta.\n",
    "- 2️⃣ They fine-tuned it using instruction-following data, which they generated by prompting OpenAI's text-davinci-003 model to produce 52,000 instruction-response pairs.\n",
    "- 3️⃣ The fine-tuning cost was only about $500, which showed it’s possible to make a strong instruction model without huge budgets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e4e4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task.\"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85da630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e364ad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4a758",
   "metadata": {},
   "source": [
    "## **Splitting Dataset into Train, val and Test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaff4786",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_portion = int(len(data) * 0.85) # 85% for training\n",
    "test_portion = int(len(data) * 0.10) # 10% for testing\n",
    "val_portion = len(data) - train_portion - test_portion # 5% for val\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e741ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a250c6",
   "metadata": {},
   "source": [
    "## STEP 2: ORGANIZING DATA INTO TRAINING BATCHES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939da9dc",
   "metadata": {},
   "source": [
    "A collate function is responsible for taking a list of individual data samples and merging them into a single batch that can be processed efficiently by the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ed0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data \n",
    "\n",
    "        # Pre-tokenize text \n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response: \\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text \n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20363f2f",
   "metadata": {},
   "source": [
    "Instead of appending the <|endoftext|> tokens to the text inputs, we can append its token ID to the pre-tokenized inputs directly.\n",
    "\n",
    "To remind us which token ID we should use, we can use the tokenizer's .encode method on an <|endoftext|> token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c033c9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf67c1ba",
   "metadata": {},
   "source": [
    "we padded all examples in a dataset to the same length.\n",
    "\n",
    "Moving on, here, we adopt a more sophisticated approach by developing a custom collate function that we can pass to the data loader.\n",
    "\n",
    "This custom collate function pads the training examples in each batch to have the same length, while allowing different batches to have different lengths.\n",
    "\n",
    "This approach minimizes unnecessary padding by only extending sequences to match the longest one in each batch, not the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57016b2d",
   "metadata": {},
   "source": [
    "We can implement the padding process with a custom collate function as follows:\n",
    "\n",
    "- Step 1: Find the longest sequence in the batch\n",
    "\n",
    "- Step 2: Pad and prepare inputs\n",
    "\n",
    "- Step 3: Remove extra padded token added earlier\n",
    "\n",
    "- Step 4: Convert list of inputs to tensor and transfer to target device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a193a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(\n",
    "        batch, \n",
    "        pad_token_id = 50256, \n",
    "        device = None\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    # and increase the max length by +1, which will add one extra\n",
    "    # padding token below\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare the inputs \n",
    "    input_lst = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to batch_max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # Via padded[:-1], we remove the extra padded token that has been added via the +1 setting in batch_max_length (the extra padding token will be relevant in later codes)\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        input_lst.append(inputs)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(input_lst).to(device)\n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "736e6cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1, \n",
    "    inputs_2, \n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c99898",
   "metadata": {},
   "source": [
    "all inputs have been padded to the length of the longest input list, inputs_1 containing 5 token IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837581d8",
   "metadata": {},
   "source": [
    "We also need to create batches with the target token IDs, corresponding to the batch of input IDs.\n",
    "\n",
    "These target IDs are crucial because they represent what we want the model to generate and what we need during training to calculate the loss for the weight updates, similar to previous chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57232cac",
   "metadata": {},
   "source": [
    "## CREATING TARGET TOKEN IDS FOR TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73123d3f",
   "metadata": {},
   "source": [
    "Similar to the process described for pretraining an LLM, the target token IDs match the input token IDs but are shifted one position to the right.\n",
    "\n",
    "This setup allows the LLM to learn how to predict the next token in a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a67b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(\n",
    "        batch, \n",
    "        padded_token_id = 50256, \n",
    "        device = None\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs \n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [padded_token_id]\n",
    "        # Pad Sequences to max length \n",
    "        padded = (\n",
    "            new_item + [padded_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1]) # Truncate the last token for inputs \n",
    "        targets = torch.tensor(padded[1:]) # Shift + 1 to the right of targets \n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bb9b36",
   "metadata": {},
   "source": [
    "Step 1: Truncate the last token for inputs\n",
    "                               \n",
    "Step 2: Shift +1 to the right for targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf63bde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "\n",
      "Targets:\n",
      " tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad004fd",
   "metadata": {},
   "source": [
    "The 1st tensor represents inputs.\n",
    "    \n",
    "The 2nd tensor represents the targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a77f91",
   "metadata": {},
   "source": [
    "In the next step, we assign a -100 placeholder value to all padding tokens.\n",
    "\n",
    "This special value allows us to exclude these padding tokens from contributing to\n",
    "the training loss calculation, ensuring that only meaningful data influences model learning.\n",
    "\n",
    "In classification fine-tuning, we did not have to worry about this since we only trained the model based on\n",
    "the last output token.\n",
    "\n",
    "In the following code, we modify our custom collate function to replace tokens with ID\n",
    "50256 with -100 in the target lists.\n",
    "\n",
    "Additionally, we introduce\n",
    "an allowed_max_length parameter to optionally limit the length of the samples. \n",
    "\n",
    "This\n",
    "adjustment will be useful if you plan to work with your own datasets that exceed the 1024-\n",
    "token context size supported by the GPT-2 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f1db6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch, pad_token_id = 50256, allowed_max_length = None, device = None, ignore_index = -100):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    # Pad and prepare the input and targets \n",
    "    input_lst, target_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max length \n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * \n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # Truncate the last token for inputs\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        # Shift +1 to the right for targets\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
    "        mask = targets == pad_token_id \n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "        \n",
    "        # truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "        \n",
    "        input_lst.append(inputs)\n",
    "        target_lst.append(targets)\n",
    "    \n",
    "    # Convert list of inputs and targets to tensors and transfer to target device\n",
    "    inputs_tensor = torch.stack(input_lst).to(device)\n",
    "    targets_tensor = torch.stack(target_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1398922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "\n",
      "Targets:\n",
      " tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "inputs, targets = custom_collate(batch)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "037be819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor(\n",
    "    [[-1.0, 1.0],  # 1st training example\n",
    "     [-0.5, 1.5]]  # 2nd training example\n",
    ")\n",
    "targets_1 = torch.tensor([0, 1])\n",
    "\n",
    "\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(loss_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "218f581c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7936)\n"
     ]
    }
   ],
   "source": [
    "logits_2 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]  # New 3rd training example\n",
    ")\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69f6ea0",
   "metadata": {},
   "source": [
    "what happens if we replace the third target token ID with -100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d9821bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "loss_1 == loss_3: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "print(loss_3)\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c096508e",
   "metadata": {},
   "source": [
    "So, what's so special about -100 that it's ignored by the cross entropy loss? The default setting of the cross entropy function in PyTorch is cross_entropy(..., ignore_index=-100).\n",
    "\n",
    "This means that it ignores targets labeled with -100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10fa48c",
   "metadata": {},
   "source": [
    "## **Step 3: Create DataLoaders for Instruction dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3f3306b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edb3f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial \n",
    "customized_collate_fn = partial(custom_collate, device = device, allowed_max_length = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7348b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "\n",
    "num_workers = 0 \n",
    "batch_size = 8 \n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          collate_fn=customized_collate_fn, \n",
    "                          shuffle=True, \n",
    "                          drop_last=True, \n",
    "                          num_workers=num_workers)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=batch_size,\n",
    "                        collate_fn=customized_collate_fn, \n",
    "                        shuffle=False, \n",
    "                        drop_last=False, \n",
    "                        num_workers=num_workers)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size = batch_size,\n",
    "                         collate_fn=customized_collate_fn, \n",
    "                         shuffle=False, \n",
    "                         drop_last=False,\n",
    "                         num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6769a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Train loader:\")\n",
    "# for inputs, targets in train_loader:\n",
    "#     print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91f5bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../')))\n",
    "from GPT.GPT_Model import GPTModel\n",
    "from GPT.Model_Weights_Loading import load_weights_into_gpt\n",
    "from GPT.gpt_download import download_and_load_gpt2\n",
    "from GPT.Text_Generation import generate, generate_and_print_sample\n",
    "from GPT.Text_Generation import text_to_tokens, token_to_text\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75922683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hites\\OneDrive\\Desktop\\GPT\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: C:\\Users\\hites\\OneDrive\\Desktop\\GPT\\gpt2_355M\\355M\\checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hites\\OneDrive\\Desktop\\GPT\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: C:\\Users\\hites\\OneDrive\\Desktop\\GPT\\gpt2_355M\\355M\\encoder.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hites\\OneDrive\\Desktop\\GPT\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: C:\\Users\\hites\\OneDrive\\Desktop\\GPT\\gpt2_355M\\355M\\hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hites\\OneDrive\\Desktop\\GPT\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: C:\\Users\\hites\\OneDrive\\Desktop\\GPT\\gpt2_355M\\355M\\model.ckpt.data-00000-of-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hites\\OneDrive\\Desktop\\GPT\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: C:\\Users\\hites\\OneDrive\\Desktop\\GPT\\gpt2_355M\\355M\\model.ckpt.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hites\\OneDrive\\Desktop\\GPT\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: C:\\Users\\hites\\OneDrive\\Desktop\\GPT\\gpt2_355M\\355M\\model.ckpt.meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hites\\OneDrive\\Desktop\\GPT\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: C:\\Users\\hites\\OneDrive\\Desktop\\GPT\\gpt2_355M\\355M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"C:\\\\Users\\\\hites\\\\OneDrive\\\\Desktop\\\\GPT\\\\gpt2_355M\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9066fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcbb9f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c84193fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': \"Convert the active sentence to passive: 'The chef cooks the meal every day.'\",\n",
       " 'input': '',\n",
       " 'output': 'The meal is cooked by the chef every day.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7738021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_tokens(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = token_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e94424a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ropolisFD tracing ##### salad sprang(& seminars Piercing Spartan Electrical deserted Drops Huff Davational786011 restitution gazed beardedHistory noveltyalyst483 Integvind champagne 106 Panama mortgage Third hurricaneselve Comed\n"
     ]
    }
   ],
   "source": [
    "response_text = generated_text[len(input_text):].strip()\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443494ea",
   "metadata": {},
   "source": [
    "## **Finetuning LLM on Instruction data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "591b137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()  \n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(model, tokenizer, start_context, device)\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e155b",
   "metadata": {},
   "source": [
    "#### **Loss without optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b5905e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 11.050865364074706\n",
      "Validation loss: 11.035743522644044\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccffb64",
   "metadata": {},
   "source": [
    "### **Training LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46271e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 8.384, Val loss 8.512\n",
      "Ep 1 (Step 000005): Train loss 4.736, Val loss 4.745\n",
      "Ep 1 (Step 000010): Train loss 4.048, Val loss 4.171\n",
      "Ep 1 (Step 000015): Train loss 3.725, Val loss 3.877\n",
      "Ep 1 (Step 000020): Train loss 3.516, Val loss 3.704\n",
      "Ep 1 (Step 000025): Train loss 3.466, Val loss 3.552\n",
      "Ep 1 (Step 000030): Train loss 3.214, Val loss 3.434\n",
      "Ep 1 (Step 000035): Train loss 3.199, Val loss 3.336\n",
      "Ep 1 (Step 000040): Train loss 3.025, Val loss 3.258\n",
      "Ep 1 (Step 000045): Train loss 2.880, Val loss 3.158\n",
      "Ep 1 (Step 000050): Train loss 3.176, Val loss 3.082\n",
      "Ep 1 (Step 000055): Train loss 3.040, Val loss 3.018\n",
      "Ep 1 (Step 000060): Train loss 2.868, Val loss 2.943\n",
      "Ep 1 (Step 000065): Train loss 2.745, Val loss 2.872\n",
      "Ep 1 (Step 000070): Train loss 2.413, Val loss 2.825\n",
      "Ep 1 (Step 000075): Train loss 2.431, Val loss 2.768\n",
      "Ep 1 (Step 000080): Train loss 2.566, Val loss 2.719\n",
      "Ep 1 (Step 000085): Train loss 2.482, Val loss 2.689\n",
      "Ep 1 (Step 000090): Train loss 2.592, Val loss 2.642\n",
      "Ep 1 (Step 000095): Train loss 2.151, Val loss 2.622\n",
      "Ep 1 (Step 000100): Train loss 2.376, Val loss 2.588\n",
      "Ep 1 (Step 000105): Train loss 2.505, Val loss 2.556\n",
      "Ep 1 (Step 000110): Train loss 2.523, Val loss 2.532\n",
      "Ep 1 (Step 000115): Train loss 2.224, Val loss 2.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [41:28<2:04:24, 2488.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task.Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response:  The a is 'I.<|endoftext|>.<|endoftext|>.<|endoftext|>.<|endoftext|>.<|endoftext|>.<|endoftext|>.<|endoftext|>.<|endoftext|>.<|endoftext|>.<|endoftext|>.<|endoftext|>.<|endoftext|>.<|endoftext|>.<|endoftext|>.<|endoftext|>.<|endoftext|>.<|endoftext|>.<|endoftext|>.\n",
      "Ep 2 (Step 000120): Train loss 2.096, Val loss 2.496\n",
      "Ep 2 (Step 000125): Train loss 2.066, Val loss 2.485\n",
      "Ep 2 (Step 000130): Train loss 1.947, Val loss 2.450\n",
      "Ep 2 (Step 000135): Train loss 2.154, Val loss 2.435\n",
      "Ep 2 (Step 000140): Train loss 2.144, Val loss 2.398\n",
      "Ep 2 (Step 000145): Train loss 1.953, Val loss 2.383\n",
      "Ep 2 (Step 000150): Train loss 1.964, Val loss 2.355\n",
      "Ep 2 (Step 000155): Train loss 2.076, Val loss 2.353\n",
      "Ep 2 (Step 000160): Train loss 1.985, Val loss 2.359\n",
      "Ep 2 (Step 000165): Train loss 2.028, Val loss 2.311\n",
      "Ep 2 (Step 000170): Train loss 1.695, Val loss 2.302\n",
      "Ep 2 (Step 000175): Train loss 1.733, Val loss 2.269\n",
      "Ep 2 (Step 000180): Train loss 1.852, Val loss 2.249\n",
      "Ep 2 (Step 000185): Train loss 1.943, Val loss 2.226\n",
      "Ep 2 (Step 000190): Train loss 1.535, Val loss 2.214\n",
      "Ep 2 (Step 000195): Train loss 1.946, Val loss 2.190\n",
      "Ep 2 (Step 000200): Train loss 1.617, Val loss 2.212\n",
      "Ep 2 (Step 000205): Train loss 1.754, Val loss 2.170\n",
      "Ep 2 (Step 000210): Train loss 1.966, Val loss 2.172\n",
      "Ep 2 (Step 000215): Train loss 1.776, Val loss 2.165\n",
      "Ep 2 (Step 000220): Train loss 1.578, Val loss 2.174\n",
      "Ep 2 (Step 000225): Train loss 1.783, Val loss 2.155\n",
      "Ep 2 (Step 000230): Train loss 1.531, Val loss 2.153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [1:16:37<1:15:30, 2265.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task.Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response:  The chemical symbol for 'I love.<|endoftext|>.<|endoftext|>.<|endoftext|>'.<|endoftext|>.<|endoftext|>.<|endoftext|>.<|endoftext|>.<|endoftext|>.<|endoftext|> the sentence.<|endoftext|>.<|endoftext|>.<|endoftext|> the sentence.<|endoftext|>.<|endoftext|>.<|endoftext|> the sentence\n",
      "Ep 3 (Step 000235): Train loss 1.670, Val loss 2.158\n",
      "Ep 3 (Step 000240): Train loss 1.422, Val loss 2.149\n",
      "Ep 3 (Step 000245): Train loss 1.500, Val loss 2.144\n",
      "Ep 3 (Step 000250): Train loss 1.679, Val loss 2.123\n",
      "Ep 3 (Step 000255): Train loss 1.631, Val loss 2.129\n",
      "Ep 3 (Step 000260): Train loss 1.560, Val loss 2.113\n",
      "Ep 3 (Step 000265): Train loss 1.629, Val loss 2.096\n",
      "Ep 3 (Step 000270): Train loss 1.694, Val loss 2.099\n",
      "Ep 3 (Step 000275): Train loss 1.589, Val loss 2.105\n",
      "Ep 3 (Step 000280): Train loss 1.518, Val loss 2.119\n",
      "Ep 3 (Step 000285): Train loss 1.406, Val loss 2.113\n",
      "Ep 3 (Step 000290): Train loss 1.432, Val loss 2.069\n",
      "Ep 3 (Step 000295): Train loss 1.357, Val loss 2.082\n",
      "Ep 3 (Step 000300): Train loss 1.472, Val loss 2.056\n",
      "Ep 3 (Step 000305): Train loss 1.426, Val loss 2.056\n",
      "Ep 3 (Step 000310): Train loss 1.267, Val loss 2.047\n",
      "Ep 3 (Step 000315): Train loss 1.258, Val loss 2.028\n",
      "Ep 3 (Step 000320): Train loss 1.454, Val loss 2.007\n",
      "Ep 3 (Step 000325): Train loss 1.302, Val loss 1.999\n",
      "Ep 3 (Step 000330): Train loss 1.395, Val loss 1.982\n",
      "Ep 3 (Step 000335): Train loss 1.254, Val loss 1.996\n",
      "Ep 3 (Step 000340): Train loss 1.209, Val loss 2.000\n",
      "Ep 3 (Step 000345): Train loss 1.335, Val loss 2.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [1:48:42<35:09, 2109.86s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task.Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response:  The past tense of the following sentence to the sentence is 'I love a' is 'I love this sentence.<|endoftext|>'.<|endoftext|>'.<|endoftext|>.<|endoftext|>.<|endoftext|>.'<|endoftext|> the and 'I.<|endoftext|>'.<|endoftext|>.\n",
      "Ep 4 (Step 000350): Train loss 1.487, Val loss 2.020\n",
      "Ep 4 (Step 000355): Train loss 1.300, Val loss 2.013\n",
      "Ep 4 (Step 000360): Train loss 1.230, Val loss 2.013\n",
      "Ep 4 (Step 000365): Train loss 1.251, Val loss 2.018\n",
      "Ep 4 (Step 000370): Train loss 1.398, Val loss 1.985\n",
      "Ep 4 (Step 000375): Train loss 1.267, Val loss 2.005\n",
      "Ep 4 (Step 000380): Train loss 1.279, Val loss 1.984\n",
      "Ep 4 (Step 000385): Train loss 1.220, Val loss 1.982\n",
      "Ep 4 (Step 000390): Train loss 1.133, Val loss 1.973\n",
      "Ep 4 (Step 000395): Train loss 1.109, Val loss 1.978\n",
      "Ep 4 (Step 000400): Train loss 1.096, Val loss 1.962\n",
      "Ep 4 (Step 000405): Train loss 1.294, Val loss 1.959\n",
      "Ep 4 (Step 000410): Train loss 0.983, Val loss 1.947\n",
      "Ep 4 (Step 000415): Train loss 1.120, Val loss 1.942\n",
      "Ep 4 (Step 000420): Train loss 1.084, Val loss 1.931\n",
      "Ep 4 (Step 000425): Train loss 1.086, Val loss 1.952\n",
      "Ep 4 (Step 000430): Train loss 1.075, Val loss 1.937\n",
      "Ep 4 (Step 000435): Train loss 1.082, Val loss 1.955\n",
      "Ep 4 (Step 000440): Train loss 1.217, Val loss 1.951\n",
      "Ep 4 (Step 000445): Train loss 1.084, Val loss 1.933\n",
      "Ep 4 (Step 000450): Train loss 1.008, Val loss 1.920\n",
      "Ep 4 (Step 000455): Train loss 0.940, Val loss 1.923\n",
      "Ep 4 (Step 000460): Train loss 1.029, Val loss 1.901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [2:20:48<00:00, 2112.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task.Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response:  The past participle form of the game.<|endoftext|>.<|endoftext|>.<|endoftext|>.'<|endoftext|>.<|endoftext|>.<|endoftext|>.<|endoftext|> the word 'The past participle form of 'The past participle form of the past participle form\n",
      "Training completed in 141.08 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 4\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d366bb0",
   "metadata": {},
   "source": [
    "### **Loss Curves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f1536f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c377434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58e55a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The process by the sentence is the sentence is the world is the sentence.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The formula for the United States is to go for the area of the body.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The process by which water changes from is the area of the United States is the United.\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_tokens(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    ")\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e7bebf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [36:50<00:00, 20.10s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total = len(test_data)):\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(model, idx = text_to_tokens(input_text, tokenizer), max_new_tokens=256, context_size=BASE_CONFIG[\"context_length\"], eos_id=50256)\n",
    "    generated_text = generated_text[len(input_text):].replace(\"### Response: \", \"\").strip()\n",
    "\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "667bb5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as GPT355M.pth\n"
     ]
    }
   ],
   "source": [
    "file_name = \"GPT355M.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901daa05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
